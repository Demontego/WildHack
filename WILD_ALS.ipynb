{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autocomplete\n",
    "from fast_autocomplete import AutoComplete\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from pyaspeller import YandexSpeller\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import tqdm\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891ea36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8172206\n"
     ]
    }
   ],
   "source": [
    "search_history = pd.read_csv('search_history.csv')\n",
    "print(len(search_history['wbuser_id'].unique()))\n",
    "search_history = search_history.drop(search_history[search_history.cnt == 0].index)\n",
    "search_history.dropna(inplace = True)\n",
    "grouped = search_history[['wbuser_id', 'UQ']].groupby('wbuser_id', as_index=False).agg(' '.join) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c359524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = []\n",
    "for i in grouped['UQ']:\n",
    "    tmp = []\n",
    "    for j in i.split(' '):\n",
    "        if len(j) > 1:\n",
    "            tmp.append(j)\n",
    "    tmp = list(set(tmp))\n",
    "    fixed.append(' '.join(tmp))\n",
    "grouped['fixed'] = fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5309cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_popularity = pd.read_csv('query_popularity.csv')\n",
    "query_popularity.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178e3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129358\n"
     ]
    }
   ],
   "source": [
    "queries_initial = query_popularity['query']\n",
    "dictionary = set()\n",
    "queries = []\n",
    "for query in queries_initial:\n",
    "    for word in query.split(' '):\n",
    "        if len(word)>1:\n",
    "            dictionary.add(word)\n",
    "print(len(dictionary))\n",
    "dictionary = list(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24b9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110444\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase = False)\n",
    "vectorizer.fit_transform(dictionary)\n",
    "for i in (set.difference(set(dictionary),set(vectorizer.get_feature_names_out()))):\n",
    "    dictionary.remove(i)\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bb7ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "ans = []\n",
    "for i in range(110):\n",
    "    encoded_input = tokenizer(dictionary[i*1000:(i+1)*1000], padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = torch.squeeze(model(**encoded_input)[1])\n",
    "    ans.append(model_output)\n",
    "encoded_input = tokenizer(dictionary[110000:], padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    model_output = torch.squeeze(model(**encoded_input)[1])\n",
    "ans.append(model_output)   \n",
    "    \n",
    "queries = torch.cat(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae9c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 336987/336987 [00:18<00:00, 18300.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import hug\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class TrieNode:\n",
    "    __slots__ = ('value', 'end_of_word', 'children', 'weight')\n",
    "\n",
    "    def __init__(self, value: str, end_of_word=False):\n",
    "        self.value = value\n",
    "        self.end_of_word = end_of_word\n",
    "        self.children = {}\n",
    "        self.weight = -1\n",
    "\n",
    "    def add(self, word_part: str, *, weight: int=-1) -> None:\n",
    "        if len(word_part) == 0:\n",
    "            self.end_of_word = True\n",
    "            self.weight = weight\n",
    "            return\n",
    "\n",
    "        first_char = word_part[0]\n",
    "        node = self.children.setdefault(first_char, TrieNode(first_char))\n",
    "        node.add(word_part[1:], weight=weight)\n",
    "\n",
    "    def find_all(self, word_part: str, path: str=\"\"):\n",
    "        if self.end_of_word:\n",
    "            yield path + self.value, self.weight\n",
    "\n",
    "        if len(word_part) > 0:\n",
    "            char = word_part[0]\n",
    "            node = self.children.get(char)\n",
    "\n",
    "            if node is not None:\n",
    "                yield from node.find_all(word_part[1:], path + self.value)\n",
    "        else:\n",
    "            for node in self.children.values():\n",
    "                yield from node.find_all(\"\", path + self.value)\n",
    "\n",
    "\n",
    "with open('words.txt',encoding=\"utf-8\") as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "root = TrieNode(\"\")\n",
    "\n",
    "print('Loading words')\n",
    "for word in tqdm.tqdm(query_popularity['query']):\n",
    "    root.add(word.rstrip('\\n'), weight=1)\n",
    "\n",
    "del words\n",
    "\n",
    "\n",
    "@hug.get('/autocomplete')\n",
    "def autocomplete(string: str, hug_timer = 0):\n",
    "    split_words = string.split()\n",
    "    last_word = split_words[-1]\n",
    "    prefix = ' '.join(split_words[:-1])\n",
    "\n",
    "    suggestions = root.find_all(last_word)\n",
    "\n",
    "    full_suggestions = []\n",
    "\n",
    "    for suggestion in suggestions:\n",
    "        full_suggestions.append((\n",
    "            '{}{}'.format((prefix + ' ') if prefix else '', suggestion[0]),\n",
    "            suggestion[1],\n",
    "        ))\n",
    "\n",
    "    sorted_suggestions = sorted(\n",
    "        full_suggestions,\n",
    "        key=itemgetter(1),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'words': list(map(itemgetter(0), sorted_suggestions)),\n",
    "        'time_taken': hug_timer,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caebd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocomplete('тап')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd1fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776642b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALS():\n",
    "    def __init__(self,users, queries, dictionary):\n",
    "        \n",
    "        #self.queries = queries\n",
    "        self.users = users\n",
    "        self.dictionary = dictionary\n",
    "        svd = TruncatedSVD(n_components=15, n_iter=7, random_state=42)\n",
    "        self.queries = svd.fit_transform(queries)\n",
    "        self.vectorizer = CountVectorizer(lowercase = False)\n",
    "        self.encoded_dict = self.vectorizer.fit_transform(self.dictionary)\n",
    "        self.interaction_matrix = self.vectorizer.transform(self.users['fixed'])\n",
    "        self.interaction_matrix[self.interaction_matrix>0] = 1\n",
    "        self.users_interest = sparse.random(len(self.users),self.queries.shape[1], density = 0.1)\n",
    "    \n",
    "    def implicit_als(self, a=40, it=10, l=0.1):\n",
    "        conf = self.users_interest.toarray() * a #+ sparse.csr_matrix(np.ones(self.users_interest.shape))\n",
    "        u_s, i_s = self.users_interest.shape\n",
    "        self.X = self.users_interest\n",
    "        self.Y = self.queries\n",
    "        Y_I = sparse.eye(i_s)\n",
    "        I = sparse.eye(self.queries.shape[1])\n",
    "        lI = l * I\n",
    "        yTy = self.Y.T.dot(self.Y)\n",
    "\n",
    "        for ip in range(it):\n",
    "            for u in tqdm.tqdm(range(u_s)):\n",
    "                p_u_el = conf[u, :].copy()\n",
    "                p_u_el[p_u_el != 0] = 1.0\n",
    "                CuI = sparse.diags(conf[u, :], 0)\n",
    "                Cu = CuI + Y_I\n",
    "                yT_CuI_y=sparse.csr_matrix(self.Y).dot(CuI).dot(sparse.csr_matrix(self.Y.T))\n",
    "                yT_Cu_pu=sparse.csr_matrix(self.Y).dot(Cu).dot(sparse.csr_matrix(p_u_el).T)\n",
    "                print('huy')\n",
    "                self.X[u] = spsolve(yTy + yT_CuI_y + lI, yT_Cu_pu)\n",
    "        return\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76994655",
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(grouped[0:10000],queries[:1000,:],dictionary[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2baedda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (15,15) and requested shape (1000,1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12124/1755993686.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicit_als\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12124/302977542.py\u001b[0m in \u001b[0;36mimplicit_als\u001b[1;34m(self, a, it, l)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0myT_Cu_pu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_u_el\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'huy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspsolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTy\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0myT_CuI_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myT_Cu_pu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\techno_w\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__radd__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__radd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# other + self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# self - other\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\techno_w\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\techno_w\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    178\u001b[0m            [1, 2, 3]])\n\u001b[0;32m    179\u001b[0m     \"\"\"\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\techno_w\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    121\u001b[0m                          'negative')\n\u001b[0;32m    122\u001b[0m     \u001b[0mextras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     it = np.nditer(\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'refs_ok'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zerosize_ok'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         op_flags=['readonly'], itershape=shape, order='C')\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (15,15) and requested shape (1000,1000)"
     ]
    }
   ],
   "source": [
    "als.implicit_als()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167eb402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
